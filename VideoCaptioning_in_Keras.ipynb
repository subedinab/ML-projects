{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtQjnHiUuQy76oSLqgWsfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subedinab/ML-projects/blob/master/VideoCaptioning_in_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGBntqctC7QD",
        "outputId": "7cc5242e-fb81-40ec-d5c3-9dc01cadb2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Video-Captioning'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 132 (delta 32), reused 72 (delta 18), pack-reused 45\u001b[K\n",
            "Receiving objects: 100% (132/132), 135.50 MiB | 22.80 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n",
            "Updating files: 100% (45/45), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Shreyz-max/Video-Captioning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Video-Captioning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYvEhM7rEjRP",
        "outputId": "865f0979-7549-4a8b-bf46-77e0e5c27dd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Video-Captioning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xkq1ciHE6pB",
        "outputId": "c12d8c8e-c6c9-48d7-e705-8451283ba731"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==0.11.0 (from -r requirements.txt (line 1))\n",
            "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.6.3)\n",
            "Collecting cachetools==4.2.1 (from -r requirements.txt (line 3))\n",
            "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
            "Collecting certifi==2020.12.5 (from -r requirements.txt (line 4))\n",
            "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "Collecting chardet==4.0.0 (from -r requirements.txt (line 5))\n",
            "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "Collecting dill==0.3.3 (from -r requirements.txt (line 6))\n",
            "  Using cached dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "Collecting flatbuffers==1.12 (from -r requirements.txt (line 7))\n",
            "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting future==0.18.2 (from -r requirements.txt (line 8))\n",
            "  Using cached future-0.18.2.tar.gz (829 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gast==0.3.3 (from -r requirements.txt (line 9))\n",
            "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting google-auth==1.27.0 (from -r requirements.txt (line 10))\n",
            "  Using cached google_auth-1.27.0-py2.py3-none-any.whl (135 kB)\n",
            "Collecting google-auth-oauthlib==0.4.2 (from -r requirements.txt (line 11))\n",
            "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.2.0)\n",
            "Collecting grpcio==1.32.0 (from -r requirements.txt (line 13))\n",
            "  Using cached grpcio-1.32.0.tar.gz (20.8 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h5py==2.10.0 (from -r requirements.txt (line 14))\n",
            "  Using cached h5py-2.10.0.tar.gz (301 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting idna==2.10 (from -r requirements.txt (line 15))\n",
            "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "Collecting importlib-metadata==3.7.0 (from -r requirements.txt (line 16))\n",
            "  Using cached importlib_metadata-3.7.0-py3-none-any.whl (11 kB)\n",
            "Collecting joblib==1.0.1 (from -r requirements.txt (line 17))\n",
            "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "Collecting Keras==2.4.3 (from -r requirements.txt (line 18))\n",
            "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting Keras-Preprocessing==1.1.2 (from -r requirements.txt (line 19))\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting Markdown==3.3.4 (from -r requirements.txt (line 20))\n",
            "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "Collecting numpy==1.19.5 (from -r requirements.txt (line 21))\n",
            "  Using cached numpy-1.19.5.zip (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting oauthlib==3.1.0 (from -r requirements.txt (line 22))\n",
            "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (3.3.0)\n",
            "Collecting protobuf==3.15.3 (from -r requirements.txt (line 24))\n",
            "  Using cached protobuf-3.15.3-py2.py3-none-any.whl (173 kB)\n",
            "Collecting pyasn1==0.4.8 (from -r requirements.txt (line 25))\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting pyasn1-modules==0.2.8 (from -r requirements.txt (line 26))\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting requests==2.25.1 (from -r requirements.txt (line 27))\n",
            "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "Collecting requests-oauthlib==1.3.0 (from -r requirements.txt (line 28))\n",
            "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting rsa==4.7.2 (from -r requirements.txt (line 29))\n",
            "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting scikit-learn==0.23.2 (from -r requirements.txt (line 30))\n",
            "  Using cached scikit-learn-0.23.2.tar.gz (7.2 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict_realtime.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANX7qE9UJQHT",
        "outputId": "464d62e4-57b3-4e41-d6a7-9c2e5caedf98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-04 10:04:11.627682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-04 10:04:13.015794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Video-Captioning/predict_realtime.py\", line 210, in <module>\n",
            "    video_to_text = VideoDescriptionRealTime(config)\n",
            "  File \"/content/Video-Captioning/predict_realtime.py\", line 26, in __init__\n",
            "    self.tokenizer, self.inf_encoder_model, self.inf_decoder_model = model.inference_model()\n",
            "  File \"/content/Video-Captioning/model.py\", line 11, in inference_model\n",
            "    tokenizer = joblib.load(file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\", line 648, in load\n",
            "    obj = _unpickle(fobj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\", line 577, in _unpickle\n",
            "    obj = unpickler.load()\n",
            "  File \"/usr/lib/python3.10/pickle.py\", line 1213, in load\n",
            "    dispatch[key[0]](self)\n",
            "  File \"/usr/lib/python3.10/pickle.py\", line 1529, in load_global\n",
            "    klass = self.find_class(module, name)\n",
            "  File \"/usr/lib/python3.10/pickle.py\", line 1580, in find_class\n",
            "    __import__(module, level=0)\n",
            "ModuleNotFoundError: No module named 'keras_preprocessing'\n"
          ]
        }
      ]
    }
  ]
}